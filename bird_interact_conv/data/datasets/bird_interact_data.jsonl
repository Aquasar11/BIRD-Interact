{"instance_id": "alien_1", "selected_database": "alien", "query": "I want to analyze how the Signal-to-Noise Quality Indicator (SNQI) varies across different weather conditions. For each weather condition, give weather condition name, the average SNQI, the median SNQI, and count how many analyzable signals there are. Sort the result by average SNQI in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["/* \nIntent: Analyze signal quality (SNQI) with atmospheric conditions using window functions\nKnowledge Used: SNQI (id:0)\nAdvanced Features: Window functions, conditional aggregation\n*/\nWITH signal_quality AS (\n    -- Step 1: Calculate SNQI for all signals with their observation conditions\n    SELECT \n        s.SignalRegistry,\n        s.SnrRatio - 0.1 * ABS(s.NoiseFloorDbm) AS SNQI,\n        o.WeathProfile\n    FROM Signals s\n    JOIN Telescopes t ON s.TelescRef = t.TelescRegistry\n    JOIN Observatories o ON t.ObservStation = o.ObservStation\n)\n-- Step 2: Rank signals by quality within weather profiles\nSELECT \n    WeathProfile,\n    AVG(SNQI) AS avg_snqi,\n    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY SNQI) AS median_snqi,\n    COUNT(*) FILTER (WHERE SNQI > 0) AS analyzable_signals\nFROM signal_quality\nGROUP BY WeathProfile\nORDER BY avg_snqi DESC;"], "external_knowledge": [0, 50], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false}, "amb_user_query": "I want to analyze how the signal quality varies across different atmospheric conditions. For each condition, give condition name, the average quality value, the median quality value, and count how many usable signals there are. Sort the result by average value.", "user_query_ambiguity": {"critical_ambiguity": [{"term": "signal quality", "sql_snippet": "s.SnrRatio - 0.1 * ABS(s.NoiseFloorDbm) AS SNQI", "is_mask": true, "type": "knowledge_linking_ambiguity"}, {"term": "good signals", "sql_snippet": "COUNT(*) FILTER (WHERE SNQI > 0) as analyzable signals", "is_mask": true, "type": "knowledge_linking_ambiguity"}], "non_critical_ambiguity": [{"term": "order", "sql_snippet": "ORDER BY avg_snqi DESC", "is_mask": false, "type": "sort_ambiguity"}]}, "knowledge_ambiguity": [{"term": "Signal-to-Noise Quality Indicator (SNQI)", "sql_snippet": "s.SnrRatio - 0.1 * ABS(s.NoiseFloorDbm) AS SNQI", "is_mask": false, "type": "knowledge_ambiguity", "deleted_knowledge": 0}], "follow_up": {"query": "How does the Atmospheric Observability Index (AOI) vary across different weather conditions? For each weather condition, provide: Weather condition name, Average AOI, Median AOI, Count of valid observations (AOI > 0), Sorted by average AOI in descending order.", "sol_sql": "/*\nIntent: Analyze Atmospheric Observability Index (AOI) with weather conditions\nKnowledge Used: AOI (id:1)\nAdvanced Features: Window functions, conditional aggregation\n*/\n\nWITH observ_conditions AS (\n    -- Step 1: Calculate AOI for all signals with their observation conditions\n    SELECT \n        s.SignalRegistry,\n        o.AtmosTransparency * (1 - o.HumidityRate / 100) * (1 - 0.02 * o.WindSpeedMs) AS AOI,\n        o.WeathProfile\n    FROM \n        Signals s\n    JOIN \n        Telescopes t ON s.TelescRef = t.TelescRegistry\n    JOIN \n        Observatories o ON t.ObservStation = o.ObservStation\n)\n\n-- Step 2: Aggregate AOI metrics by weather profile\nSELECT \n    WeathProfile,\n    AVG(AOI) AS avg_aoi,\n    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY AOI) AS median_aoi,\n    COUNT(*) FILTER (WHERE AOI > 0) AS valid_observations\nFROM \n    observ_conditions\nGROUP BY \n    WeathProfile\nORDER BY \n    avg_aoi DESC;", "external_knowledge": [1], "type": "attribute_change", "test_cases": [], "category": "Query", "difficulty_tier": "Medium"}, "difficulty_tier": "Medium"}
{"instance_id": "alien_2", "selected_database": "alien", "query": "Classify signals by TOLS Category, and for each group, show the category name, signal count, average Bandwidth-to-Frequency Ratio, and the standard deviation of the anomaly score.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["/*\nIntent: Analyze distribution of technosignature probabilities using statistical functions\nKnowledge Used: TOLS (id:3)\nAdvanced Features: Statistical functions, CASE expressions\n*/\n-- Step 1: Calculate TOLS for all signals with probability data\nSELECT \n    -- Step 2: Bin the TOLS values into categories\n    CASE \n        WHEN p.TechSigProb * (1 - p.NatSrcProb) * p.SigUnique * (0.5 + p.AnomScore/10) < 0.25 THEN 'Low'\n        WHEN p.TechSigProb * (1 - p.NatSrcProb) * p.SigUnique * (0.5 + p.AnomScore/10) < 0.75 THEN 'Medium'\n        ELSE 'High'\n    END AS tol_category,\n    -- Step 3: Calculate statistics for each category\n    COUNT(*) AS signal_count,\n    AVG(s.BwHz/(s.CenterFreqMhz * 1000000)) AS avg_bfr,\n    STDDEV(p.AnomScore) AS anomaly_stddev\nFROM Signals s\nJOIN SignalProbabilities p ON s.SignalRegistry = p.SignalRef\nGROUP BY tol_category;"], "external_knowledge": [3, 51, 52], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false}, "amb_user_query": "Classify signals by their score level, and for each group, show the classification, signal count, average BFR measure, and the standard deviation of the anomaly metric.", "user_query_ambiguity": {"critical_ambiguity": [{"term": "score level", "sql_snippet": "CASE WHEN p.TechSigProb * (1 - p.NatSrcProb) * p.SigUnique * (0.5 + p.AnomScore/10) < 0.25 THEN 'Low' WHEN ... END as tol_catagory", "is_mask": true, "type": "knowledge_linking_ambiguity"}, {"term": "BFR measure", "sql_snippet": "AVG(s.BwHz/(s.CenterFreqMhz * 1000000)) as Bandwidth-to-Frequency Ratio", "is_mask": true, "type": "knowledge_linking_ambiguity"}, {"term": "anomaly metric", "sql_snippet": "STDDEV(p.AnomScore) as anomaly score", "is_mask": false, "type": "schema_linking_ambiguity"}], "non_critical_ambiguity": []}, "knowledge_ambiguity": [{"term": "TOLS Category", "sql_snippet": "COUNT(*) AS signal_count,AVG(s.BwHz / (s.CenterFreqMhz * 1000000)) AS avg_bfr,STDDEV(p.AnomScore) AS anomaly_stddev", "is_mask": true, "type": "knowledge_ambiguity", "deleted_knowledge": 52}], "follow_up": {"query": "For the TOLS category with the highest signal count, calculate the average, minimum, and maximum Signal Stability Metric (SSM) for the signals in that category.", "sol_sql": "/*\nIntent: Drill down into the TOLS category with the highest signal count to analyze signal stability\nKnowledge Used: TOLS (id:3), TOLS Category (id:52), SSM (id:7)\nAdvanced Features: Subquery to identify the category with the most signals, statistical functions\n*/\nWITH tol_counts AS (\n    -- Step 1: Recompute the first-round query to find the category with the most signals\n    SELECT \n        CASE \n            WHEN p.TechSigProb * (1 - p.NatSrcProb) * p.SigUnique * (0.5 + p.AnomScore/10) < 0.25 THEN 'Low'\n            WHEN p.TechSigProb * (1 - p.NatSrcProb) * p.SigUnique * (0.5 + p.AnomScore/10) < 0.75 THEN 'Medium'\n            ELSE 'High'\n        END AS tol_category,\n        COUNT(*) AS signal_count\n    FROM Signals s\n    JOIN SignalProbabilities p ON s.SignalRegistry = p.SignalRef\n    GROUP BY tol_category\n    ORDER BY signal_count DESC\n    LIMIT 1\n),\nsignal_stability AS (\n    -- Step 2: Calculate SSM for signals in the top category\n    SELECT \n        s.SignalRegistry,\n        (1 - ABS(s.FreqDriftHzs) / (s.FreqMhz * 1000)) * (s.SigDurSec / (1 + s.DoppShiftHz / 1000)) AS ssm\n    FROM Signals s\n    JOIN SignalProbabilities p ON s.SignalRegistry = p.SignalRef\n    JOIN tol_counts tc ON \n        CASE \n            WHEN p.TechSigProb * (1 - p.NatSrcProb) * p.SigUnique * (0.5 + p.AnomScore/10) < 0.25 THEN 'Low'\n            WHEN p.TechSigProb * (1 - p.NatSrcProb) * p.SigUnique * (0.5 + p.AnomScore/10) < 0.75 THEN 'Medium'\n            ELSE 'High'\n        END = tc.tol_category\n)\n-- Step 3: Aggregate SSM statistics for the top category\nSELECT \n    AVG(ssm) AS avg_ssm,\n    MIN(ssm) AS min_ssm,\n    MAX(ssm) AS max_ssm\nFROM signal_stability;", "external_knowledge": [7], "type": "result_based", "test_cases": [], "category": "Query", "difficulty_tier": "Hard"}, "difficulty_tier": "Easy"}
{"instance_id": "alien_3", "selected_database": "alien", "query": "Analyze how lunar interference affects observations by showing the current moon phase, average Lunar Interference Factor (LIF) and the count of high lunar interference events for each observatory, sorted by average LIF in descending order.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["/*\nIntent: Quantify lunar interference impact using LIF calculation\nKnowledge Used: LIF (id:9)\nAdvanced Features: Date functions, lateral join\n*/\n-- Step 1: Calculate LIF for all observations with lunar data\nSELECT \n    o.ObservStation,\n    -- Step 2: Calculate average LIF by moon phase\n    o.LunarStage,\n    AVG((1 - o.LunarDistDeg/180) * (1 - o.AtmosTransparency)) AS avg_lif,\n    -- Step 3: Count high interference events\n    COUNT(*) FILTER (WHERE (1 - o.LunarDistDeg/180) * (1 - o.AtmosTransparency) > 0.5) AS high_interf_count\nFROM Observatories o\nGROUP BY o.ObservStation, o.LunarStage\nORDER BY avg_lif DESC;"], "external_knowledge": [9, 53], "test_cases": [], "category": "Query", "high_level": false, "conditions": {"decimal": -1, "distinct": false}, "amb_user_query": "Analyze how lunar interference affects observations by showing the current moon phase, average interference level and the count of problematic events for each observatory, sorted by average interference.", "user_query_ambiguity": {"critical_ambiguity": [{"term": "interference level", "sql_snippet": "AVG((1 - o.LunarDistDeg/180) * (1 - o.AtmosTransparency)) AS avg_lif", "is_mask": true, "type": "knowledge_linking_ambiguity"}, {"term": "problematic events", "sql_snippet": "COUNT(*) FILTER (WHERE (1 - o.LunarDistDeg/180) * (1 - o.AtmosTransparency) > 0.5) as High Lunar Interference Events", "is_mask": true, "type": "knowledge_linking_ambiguity"}], "non_critical_ambiguity": [{"term": "sorted by average interference", "sql_snippet": "ORDER BY avg_lif DESC", "is_mask": false, "type": "sort_ambiguity"}]}, "knowledge_ambiguity": [{"term": "Lunar Interference Factor (LIF)", "sql_snippet": "AVG((1 - o.LunarDistDeg / 180) * (1 - o.AtmosTransparency)) AS avg_lif", "is_mask": false, "type": "knowledge_ambiguity", "deleted_knowledge": 9}], "follow_up": {"query": "Instead of grouping by moon phase, group the results by geomagnetic status, and show the geomagnetic status and other same metrics for each observatory.", "sol_sql": "/*\nIntent: Quantify lunar interference impact by switching grouping to geomagnetic status\nKnowledge Used: LIF (id:9)\nAdvanced Features: Date functions, lateral join\n*/\n-- Step 1: Calculate LIF for all observations with lunar data\nSELECT \n    o.ObservStation,\n    -- Step 2: Calculate average LIF by geomagnetic status instead of lunar stage\n    o.GeomagStatus,\n    AVG((1 - o.LunarDistDeg/180) * (1 - o.AtmosTransparency)) AS avg_lif,\n    -- Step 3: Count high interference events\n    COUNT(*) FILTER (WHERE (1 - o.LunarDistDeg/180) * (1 - o.AtmosTransparency) > 0.5) AS high_interf_count\nFROM Observatories o\nGROUP BY o.ObservStation, o.GeomagStatus\nORDER BY avg_lif DESC;", "external_knowledge": [], "type": "topic_pivot", "test_cases": [], "category": "Query", "difficulty_tier": "Easy"}, "difficulty_tier": "Easy"}
{"instance_id": "alien_4", "selected_database": "alien", "query": "Which observatory stations are discovering the most potential technosignatures? For each station, display the observatory name, how many signals meet our technosignature criteria, their average TOLS score, average BFR values, and what percentage of all detected technosignatures they've found. I need this ranked by the stations with the most discoveries first.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["/*\nIntent: Evaluate detection efficiency for potential technosignatures\nKnowledge Used: Technosignature (id:10), TOLS (id:3)\nAdvanced Features: Common Table Expressions, complex filtering\n*/\nWITH tech_signals AS (\n    SELECT \n        s.SignalRegistry,\n        s.TelescRef,\n        p.TechSigProb * (1 - p.NatSrcProb) * p.SigUnique * (0.5 + p.AnomScore/10) AS TOLS,\n        s.BwHz/(s.CenterFreqMhz * 1000000) AS BFR\n    FROM Signals s\n    JOIN SignalProbabilities p ON s.SignalRegistry = p.SignalRef\n    WHERE p.TechSigProb > 0.7 \n      AND p.NatSrcProb < 0.3 \n      AND p.ArtSrcProb < 50\n      AND s.BwHz/(s.CenterFreqMhz * 1000000) < 0.001\n)\nSELECT \n    t.ObservStation,\n    COUNT(*) AS potential_tech_signals,\n    AVG(ts.TOLS) AS avg_tols,\n    AVG(ts.BFR) AS avg_bfr,\n    COUNT(*) * 100.0 / (SELECT COUNT(*) FROM tech_signals) AS percentage_of_total\nFROM tech_signals ts\nJOIN Telescopes t ON ts.TelescRef = t.TelescRegistry\nGROUP BY t.ObservStation\nORDER BY potential_tech_signals DESC;"], "external_knowledge": [10, 3], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false}, "amb_user_query": "Which observatory stations are discovering the most potential candidate signals? For each station, display the observatory name, how many signals meet our detection thresholds, their average origin score, average frequency ratios, and what percentage of all detected candidates they've found. I need this ranked.", "user_query_ambiguity": {"critical_ambiguity": [{"term": "candidate signals", "sql_snippet": "WHERE p.TechSigProb > 0.7 AND p.NatSrcProb < 0.3 AND p.ArtSrcProb < 50 AND s.BwHz/(s.CenterFreqMhz * 1000000) < 0.001", "is_mask": true, "type": "knowledge_linking_ambiguity"}, {"term": "detection thresholds", "sql_snippet": "p.TechSigProb > 0.7 AND p.NatSrcProb < 0.3 AND p.ArtSrcProb < 50", "is_mask": true, "type": "semantic_ambiguity"}, {"term": "origin score", "sql_snippet": "p.TechSigProb * (1 - p.NatSrcProb) * p.SigUnique * (0.5 + p.AnomScore/10) AS TOLS", "is_mask": true, "type": "knowledge_linking_ambiguity"}], "non_critical_ambiguity": [{"term": "rank", "sql_snippet": "ORDER BY potential_tech_signals DESC", "is_mask": false, "type": "sort_ambiguity"}]}, "knowledge_ambiguity": [{"term": "Technological Origin Likelihood Score (TOLS)", "sql_snippet": "p.TechSigProb * (1 - p.NatSrcProb) * p.SigUnique * (0.5 + p.AnomScore/10) AS TOLS", "is_mask": false, "type": "knowledge_ambiguity", "deleted_knowledge": 3}], "follow_up": {"query": "Replace the average TOLS score with the average SNQI and the average BFR with the average ECI for each observatory station in the output. Keep other outputs same as before.", "sol_sql": "/*\nIntent: Evaluate detection efficiency for potential technosignatures with modified metrics\nKnowledge Used: Technosignature (id:10), SNQI (id:0), ECI (id:6)\nAdvanced Features: Common Table Expressions, complex filtering\n*/\nWITH tech_signals AS (\n    SELECT \n        s.SignalRegistry,\n        s.TelescRef,\n        (s.SnrRatio - 0.1 * ABS(s.NoiseFloorDbm)) AS SNQI,  -- Calculate SNQI\n        sd.CompressRatio * sc.ComplexIdx * sc.EntropyVal / 10 AS ECI  -- Calculate ECI\n    FROM Signals s\n    JOIN SignalProbabilities p ON s.SignalRegistry = p.SignalRef\n    JOIN SignalDecoding sd ON s.SignalRegistry = sd.SignalRef\n    JOIN SignalClassification sc ON s.SignalRegistry = sc.SignalRef\n    WHERE p.TechSigProb > 0.7 \n      AND p.NatSrcProb < 0.3 \n      AND p.ArtSrcProb < 50\n      AND s.BwHz/(s.CenterFreqMhz * 1000000) < 0.001\n)\nSELECT \n    t.ObservStation,\n    COUNT(*) AS potential_tech_signals,\n    AVG(ts.SNQI) AS avg_snqi,  -- Replace avg_tols with avg_snqi\n    AVG(ts.ECI) AS avg_eci,    -- Replace avg_bfr with avg_eci\n    COUNT(*) * 100.0 / (SELECT COUNT(*) FROM tech_signals) AS percentage_of_total\nFROM tech_signals ts\nJOIN Telescopes t ON ts.TelescRef = t.TelescRegistry\nGROUP BY t.ObservStation\nORDER BY potential_tech_signals DESC;", "external_knowledge": [6], "type": "attribute_change", "test_cases": [], "category": "Query", "difficulty_tier": "Hard"}, "difficulty_tier": "Medium"}
{"instance_id": "alien_5", "selected_database": "alien", "query": "Show me a breakdown of signal modulation types with at least 5 occurrences. For each modulation type, display the modulation type, the number of signals, the average Modulation Complexity Score (MCS), and average signal-to-noise ratio (SNR). Also include a detailed JSON with each signal's MCS and SNR values. Keys are the signal record IDs, and values are inner objects containing two fields: `mcs` – the signal’s MCS value, and `snr` – the SNR value.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["/*\nIntent: Evaluate effectiveness of different modulation types\nKnowledge Used: MCS (id:30)\nAdvanced Features: JSON functions, advanced aggregation\n*/\nSELECT \n    s.ModType,\n    COUNT(*) AS signal_count,\n    AVG(s.ModIndex * (1 + (1 - ABS(s.FreqDriftHzs)/(s.FreqMhz*1000)) * \n        s.SigDurSec/(1 + s.DoppShiftHz/1000)) *\n        CASE \n            WHEN s.ModType = 'AM' THEN 2\n            WHEN s.ModType = 'FM' THEN 1.5\n            ELSE 1\n        END) AS avg_mcs,\n    AVG(s.SnrRatio) AS avg_snr,\n    JSON_OBJECT_AGG(\n        s.SignalRegistry,\n        JSON_BUILD_OBJECT(\n            'mcs', s.ModIndex * (1 + (1 - ABS(s.FreqDriftHzs)/(s.FreqMhz*1000)) * \n                   s.SigDurSec/(1 + s.DoppShiftHz/1000)) *\n                   CASE \n                       WHEN s.ModType = 'AM' THEN 2\n                       WHEN s.ModType = 'FM' THEN 1.5\n                       ELSE 1\n                   END,\n            'snr', s.SnrRatio\n        )\n    ) AS signal_details\nFROM Signals s\nWHERE s.ModType IS NOT NULL\nGROUP BY s.ModType\nHAVING COUNT(*) > 5;"], "external_knowledge": [30], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false}, "amb_user_query": "Show me a breakdown of signal encoding methods with several occurrences. For each method, display the type, the count, the average complexity measure, and average quality ratio. Also include signal details.", "user_query_ambiguity": {"critical_ambiguity": [{"term": "signal encoding methods", "sql_snippet": "s.ModType", "is_mask": false, "type": "schema_linking_ambiguity"}, {"term": "complexity measure", "sql_snippet": "AVG(s.ModIndex * (1 + (1 - ABS(s.FreqDriftHzs)/(s.FreqMhz*1000)) * s.SigDurSec/(1 + s.DoppShiftHz/1000)) * CASE WHEN s.ModType = 'AM' THEN 2 WHEN s.ModType = 'FM' THEN 1.5 ELSE 1 END", "is_mask": true, "type": "knowledge_linking_ambiguity"}, {"term": "quality ratio", "sql_snippet": "AVG(s.SnrRatio)", "is_mask": false, "type": "semantic_ambiguity"}, {"term": "signal details", "sql_snippet": "JSON_OBJECT_AGG(s.SignalRegistry,JSON_BUILD_OBJECT('mcs', s.ModIndex * (1 + (1 - ABS(s.FreqDriftHzs) / (s.FreqMhz * 1000)) * s.SigDurSec / (1 + s.DoppShiftHz / 1000) *CASE WHEN s.ModType = 'AM' THEN 2WHEN s.ModType = 'FM' THEN 1.5ELSE 1END,'snr', s.SnrRatio)) AS signal_details", "is_mask": true, "type": "semantic_ambiguity"}], "non_critical_ambiguity": [{"term": "null", "sql_snippet": "WHERE s.ModType IS NOT NULL", "is_mask": false, "type": "null_ambiguity"}]}, "knowledge_ambiguity": [{"term": "Modulation Complexity Score (MCS)", "sql_snippet": "AVG(s.ModIndex * (1 + (1 - ABS(s.FreqDriftHzs)/(s.FreqMhz*1000)) * s.SigDurSec/(1 + s.DoppShiftHz/1000)) * CASE WHEN s.ModType = 'AM' THEN 2 WHEN s.ModType = 'FM' THEN 1.5 ELSE 1 END) AS avg_mcs", "is_mask": false, "type": "knowledge_ambiguity", "deleted_knowledge": 30}], "follow_up": {"query": "Filter the breakdown to include only analyzable signals, while still showing other metrics", "sol_sql": "/*\nIntent: Evaluate effectiveness of different modulation types with additional constraint\nKnowledge Used: MCS (id:30), SSM (id:7), Analyzable Signals (id:50), SNQI (id:0)\nAdvanced Features: JSON functions, advanced aggregation\n*/\nSELECT \n    s.ModType,\n    COUNT(*) AS signal_count,\n    AVG(s.ModIndex * (1 + (1 - ABS(s.FreqDriftHzs)/(s.FreqMhz*1000)) * \n        s.SigDurSec/(1 + s.DoppShiftHz/1000)) *\n        CASE \n            WHEN s.ModType = 'AM' THEN 2\n            WHEN s.ModType = 'FM' THEN 1.5\n            ELSE 1\n        END) AS avg_mcs,\n    AVG(s.SnrRatio) AS avg_snr,\n    JSON_OBJECT_AGG(\n        s.SignalRegistry,\n        JSON_BUILD_OBJECT(\n            'mcs', s.ModIndex * (1 + (1 - ABS(s.FreqDriftHzs)/(s.FreqMhz*1000)) * \n                   s.SigDurSec/(1 + s.DoppShiftHz/1000)) *\n                   CASE \n                       WHEN s.ModType = 'AM' THEN 2\n                       WHEN s.ModType = 'FM' THEN 1.5\n                       ELSE 1\n                   END,\n            'snr', s.SnrRatio\n        )\n    ) AS signal_details\nFROM Signals s\nWHERE s.ModType IS NOT NULL\n  AND (s.SnrRatio - 0.1 * ABS(s.NoiseFloorDbm)) > 0  -- SNQI > 0 for analyzable signals\nGROUP BY s.ModType\nHAVING COUNT(*) > 5;", "external_knowledge": [50], "type": "constraint_change", "test_cases": [], "category": "Query", "difficulty_tier": "Medium"}, "difficulty_tier": "Medium"}
{"instance_id": "alien_6", "selected_database": "alien", "query": "Which observatories have the most promising signals worth investigating? I need a summary by observatory showing the observatory name, their total signal count, average Research Priority Index (RPI) and approximate Confirmation Confidence Score (CCS) values, number of high-priority (RPI > 3) signals, number of high-confidence signals, and especially the number of signals that meet both criteria. Sort the results by observatories with the most high-priority-and-high-confidence signals first.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["/*\nIntent: Optimize research priorities based on multiple factors\nKnowledge Used: RPI (id:8), CCS (id:36)\nAdvanced Features: Complex calculations, window functions\n*/\nWITH priority_calc AS (\n    SELECT \n        s.SignalRegistry,\n        (p.TechSigProb * 4 + p.BioSigProb/100 + p.SigUnique * 2 + p.AnomScore/2) * \n            (1 - p.FalsePosProb) AS RPI,\n        (1 - p.FalsePosProb) * d.DecodeConf * \n            (CASE WHEN s.SnrRatio - 0.1 * ABS(s.NoiseFloorDbm) > 0 \n                  THEN (s.SnrRatio - 0.1 * ABS(s.NoiseFloorDbm))/10 + 0.5 \n                  ELSE 0.1 END) AS CCS_approx,\n        o.ObservStation\n    FROM Signals s\n    JOIN SignalProbabilities p ON s.SignalRegistry = p.SignalRef\n    JOIN SignalDecoding d ON s.SignalRegistry = d.SignalRef\n    JOIN Telescopes t ON s.TelescRef = t.TelescRegistry\n    JOIN Observatories o ON t.ObservStation = o.ObservStation\n)\nSELECT \n    ObservStation,\n    COUNT(*) AS signal_count,\n    AVG(RPI) AS avg_rpi,\n    AVG(CCS_approx) AS avg_ccs,\n    COUNT(*) FILTER (WHERE RPI > 3) AS high_priority_signals,\n    COUNT(*) FILTER (WHERE CCS_approx > 0.8) AS high_confidence_signals,\n    COUNT(*) FILTER (WHERE RPI > 3.5 AND CCS_approx > 0.8) AS high_priority_high_confidence\nFROM priority_calc\nGROUP BY ObservStation\nORDER BY high_priority_high_confidence DESC;"], "external_knowledge": [8, 47, 54], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false}, "amb_user_query": "Which observatories have the most promising signals worth investigating? I need a summary by observatory showing the observatory name, their total signal count, average priority score and approximate Confirmation Confidence Score (CCS) values, number of important signals, number of high-confidence signals, and especially the number of signals that meet both criteria. Organize the results by observatories with the most promising signals first.", "user_query_ambiguity": {"critical_ambiguity": [{"term": "promising", "sql_snippet": "COUNT(*) FILTER (WHERE RPI > 3.5 AND CCS_approx > 0.8) AS high_priority_high_confidence\nFROM priority_calc", "is_mask": true, "type": "intent_ambiguity"}, {"term": "priority score", "sql_snippet": "(p.TechSigProb * 4 + p.BioSigProb/100 + p.SigUnique * 2 + p.AnomScore/2) * (1 - p.FalsePosProb) AS RPI", "is_mask": true, "type": "knowledge_linking_ambiguity"}, {"term": "important", "sql_snippet": "COUNT(*) FILTER (WHERE RPI > 3) AS high_priority_signals", "is_mask": false, "type": "semantic_ambiguity"}], "non_critical_ambiguity": [{"term": "organize", "sql_snippet": "ORDER BY high_priority_high_confidence DESC", "is_mask": false, "type": "sort_ambiguity"}]}, "knowledge_ambiguity": [{"term": "Research Priority Index (RPI)", "sql_snippet": "(p.TechSigProb * 4 + p.BioSigProb/100 + p.SigUnique * 2 + p.AnomScore/2)*(1 - p.FalsePosProb) AS RPI", "is_mask": true, "type": "knowledge_ambiguity", "deleted_knowledge": 8}], "follow_up": {"query": "Instead of average RPI and CCS, show the average Modulation Complexity Score (MCS) and average Technological Origin Likelihood Score (TOLS) for each observatory, while keeping other outputs and sorting order unchaged.", "sol_sql": "/*\nIntent: Optimize research priorities with new metrics\nKnowledge Used: RPI (id:8), CCS (id:36), MCS (id:30), TOLS (id:3)\nAdvanced Features: Complex calculations, window functions\n*/\nWITH priority_calc AS (\n    SELECT \n        s.SignalRegistry,\n        (p.TechSigProb * 4 + p.BioSigProb/100 + p.SigUnique * 2 + p.AnomScore/2) * \n            (1 - p.FalsePosProb) AS RPI,\n        (1 - p.FalsePosProb) * d.DecodeConf * \n            (CASE WHEN s.SnrRatio - 0.1 * ABS(s.NoiseFloorDbm) > 0 \n                  THEN (s.SnrRatio - 0.1 * ABS(s.NoiseFloorDbm))/10 + 0.5 \n                  ELSE 0.1 END) AS CCS_approx,\n        (p.TechSigProb * (1 - p.NatSrcProb) * p.SigUnique * (0.5 + p.AnomScore/10)) AS TOLS,\n        (s.ModIndex * (1 + (1 - ABS(s.FreqDriftHzs)/(s.FreqMhz*1000)) * \n            s.SigDurSec/(1 + s.DoppShiftHz/1000)) * \n            CASE \n                WHEN s.ModType = 'AM' THEN 2\n                WHEN s.ModType = 'FM' THEN 1.5\n                ELSE 1\n            END) AS MCS,\n        o.ObservStation\n    FROM Signals s\n    JOIN SignalProbabilities p ON s.SignalRegistry = p.SignalRef\n    JOIN SignalDecoding d ON s.SignalRegistry = d.SignalRef\n    JOIN Telescopes t ON s.TelescRef = t.TelescRegistry\n    JOIN Observatories o ON t.ObservStation = o.ObservStation\n)\nSELECT \n    ObservStation,\n    COUNT(*) AS signal_count,\n    AVG(MCS) AS avg_mcs,\n    AVG(TOLS) AS avg_tols,\n    COUNT(*) FILTER (WHERE RPI > 3) AS high_priority_signals,\n    COUNT(*) FILTER (WHERE CCS_approx > 0.8) AS high_confidence_signals,\n    COUNT(*) FILTER (WHERE RPI > 3.5 AND CCS_approx > 0.8) AS high_priority_high_confidence\nFROM priority_calc\nGROUP BY ObservStation\nORDER BY high_priority_high_confidence DESC;", "external_knowledge": [30], "type": "attribute_change", "test_cases": [], "category": "Query", "difficulty_tier": "Medium"}, "difficulty_tier": "Medium"}
{"instance_id": "alien_7", "selected_database": "alien", "query": "Create a report evaluating observatory conditions using the Atmospheric Observability Index (AOI) and identifying stations meeting Optimal Observing Window (OOW) criteria. The output should group stations by whether meets OOW (bool:True or False). For each group, you should provide its boolean value of OOW, the count of stations, average AOI (rounded to 3 decimal places) and a JSON array whose elements each include  `station` (observatory name), `aoi` (AOI value), `lunar_factors` object with `stage` (lunar stage) and `distance` (lunar distance), and `solar_status` (solar status).", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["-- This query evaluates optimal observatory conditions using Atmospheric Observability Index (AOI)\n-- and generates a JSON report of environmental factors using JSON aggregation functions.\n-- Using KB knowledge: Atmospheric Observability Index (AOI) [id: 1], Optimal Observing Window (OOW) [id: 13]\n\n-- Step 1: Calculate AOI for each observatory and determine if conditions meet Optimal Observing Window criteria\nWITH observatory_conditions AS (\n    SELECT \n        o.ObservStation,\n        o.AtmosTransparency * (1 - o.HumidityRate/100) * (1 - 0.02 * o.WindSpeedMs) AS aoi,  -- AOI formula from KB\n        o.LunarStage,\n        o.LunarDistDeg,\n        o.SolarStatus,\n        -- Check if conditions meet OOW criteria\n        CASE WHEN \n            o.AtmosTransparency * (1 - o.HumidityRate/100) * (1 - 0.02 * o.WindSpeedMs) > 0.85 AND\n            (o.LunarStage = 'New' OR o.LunarStage = 'First Quarter') AND\n            o.LunarDistDeg > 45 AND\n            (o.SolarStatus = 'Low' OR o.SolarStatus = 'Moderate')\n        THEN TRUE ELSE FALSE END AS is_optimal_window\n    FROM \n        Observatories o\n)\n-- Step 2: Generate JSON report grouping observatories by optimal condition status\nSELECT \n    is_optimal_window,\n    COUNT(*) AS station_count,\n    ROUND(AVG(aoi)::numeric, 3) AS avg_aoi,\n    -- Use JSON aggregation to create detailed environmental factors report\n    jsonb_agg(jsonb_build_object(\n        'station', ObservStation,\n        'aoi', ROUND(aoi::numeric, 3),\n        'lunar_factors', jsonb_build_object(\n            'stage', LunarStage,\n            'distance', LunarDistDeg\n        ),\n        'solar_status', SolarStatus\n    )) AS observatory_details\nFROM \n    observatory_conditions\nGROUP BY \n    is_optimal_window;"], "external_knowledge": [1, 13], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 3, "distinct": false}, "amb_user_query": "Create a report evaluating observatory conditions using the Observability Index and identifying stations meeting good observation conditions. The output should group stations by whether meets good observation conditions (bool:True or False). For each group, you should provide its boolean value, the count of stations, average AOI and a detailed JSON array of environmental factors containing station name, AOI value, an object called 'lunar factors' with lunar stage and lunar distance inside, and solar condition.", "user_query_ambiguity": {"critical_ambiguity": [{"term": "Observability Index", "sql_snippet": "o.AtmosTransparency * (1 - o.HumidityRate/100) * (1 - 0.02 * o.WindSpeedMs) AS aoi", "is_mask": true, "type": "knowledge_linking_ambiguity"}, {"term": "good observation conditions", "sql_snippet": "o.AtmosTransparency * (1 - o.HumidityRate/100) * (1 - 0.02 * o.WindSpeedMs) > 0.85 AND o.LunarStage IN ('New', 'First Quarter') AND o.LunarDistDeg > 45 AND o.SolarStatus IN ('Low', 'Moderate')", "is_mask": false, "type": "intent_ambiguity"}, {"term": "solar condition", "sql_snippet": "o.SolarStatus", "is_mask": false, "type": "schema_linking_ambiguity"}, {"term": "json array", "sql_snippet": "    jsonb_agg(jsonb_build_object(\n        'station', ObservStation,\n        'aoi', ROUND(aoi::numeric, 3),\n        'lunar_factors', jsonb_build_object(\n            'stage', LunarStage,\n            'distance', LunarDistDeg\n        ),\n        'solar_status', SolarStatus\n    )) AS observatory_details", "is_mask": true, "type": "intent_ambiguity"}], "non_critical_ambiguity": [{"term": "average AOI", "sql_snippet": "ROUND(AVG(aoi), 3)", "is_mask": false, "type": "decimal_ambiguity"}]}, "knowledge_ambiguity": [{"term": "Optimal Observing Window (OOW)", "sql_snippet": "o.AtmosTransparency * (1 - o.HumidityRate/100) * (1 - 0.02 * o.WindSpeedMs) > 0.85 as OOW", "is_mask": true, "type": "knowledge_ambiguity", "deleted_knowledge": 13}], "follow_up": {"query": "Modify the Optimal Observing Window (OOW) criteria by lowering the AOI threshold to 0.75, requiring the geomagnetic status to be 'Quiet', and excluding any conditions that fall under the Signal Degradation Scenario (SDS). Keep the same output format, grouping by whether the modified OOW is met, with the count of stations, average AOI, and the detailed JSON array of environmental factors.", "sol_sql": "-- This query evaluates observatory conditions with modified Optimal Observing Window (OOW) criteria\n-- Using KB knowledge: Atmospheric Observability Index (AOI) [id: 1], Optimal Observing Window (OOW) [id: 13], Signal Degradation Scenario (SDS) [id: 14]\n\n-- Step 1: Calculate AOI for each observatory and determine if conditions meet the modified Optimal Observing Window criteria\nWITH observatory_conditions AS (\n    SELECT \n        o.ObservStation,\n        o.AtmosTransparency * (1 - o.HumidityRate/100) * (1 - 0.02 * o.WindSpeedMs) AS aoi,  -- AOI formula from KB\n        o.LunarStage,\n        o.LunarDistDeg,\n        o.SolarStatus,\n        -- Modified OOW criteria: relax AOI threshold, add geomagstatus condition, and exclude SDS conditions\n        CASE WHEN \n            o.AtmosTransparency * (1 - o.HumidityRate/100) * (1 - 0.02 * o.WindSpeedMs) > 0.75 AND  -- Relaxed AOI threshold from 0.85 to 0.75\n            (o.LunarStage = 'New' OR o.LunarStage = 'First Quarter') AND\n            o.LunarDistDeg > 45 AND\n            (o.SolarStatus = 'Low' OR o.SolarStatus = 'Moderate') AND\n            o.GeomagStatus = 'Quiet' AND  -- New condition for geomagnetic status\n            NOT (  -- Exclude Signal Degradation Scenario (SDS) conditions\n                o.AtmosTransparency < 0.7 OR\n                o.HumidityRate > 70 OR\n                o.WindSpeedMs > 8 OR\n                o.GeomagStatus LIKE '%Storm%'\n            )\n        THEN TRUE ELSE FALSE END AS is_optimal_window\n    FROM \n        Observatories o\n)\n-- Step 2: Generate JSON report grouping observatories by optimal condition status\nSELECT \n    is_optimal_window,\n    COUNT(*) AS station_count,\n    ROUND(AVG(aoi)::numeric, 3) AS avg_aoi,\n    -- Use JSON aggregation to create detailed environmental factors report\n    jsonb_agg(jsonb_build_object(\n        'station', ObservStation,\n        'aoi', ROUND(aoi::numeric, 3),\n        'lunar_factors', jsonb_build_object(\n            'stage', LunarStage,\n            'distance', LunarDistDeg\n        ),\n        'solar_status', SolarStatus\n    )) AS observatory_details\nFROM \n    observatory_conditions\nGROUP BY \n    is_optimal_window;", "external_knowledge": [14], "type": "constraint_change", "test_cases": [], "category": "Query", "difficulty_tier": "Medium"}, "difficulty_tier": "Medium"}
{"instance_id": "alien_8", "selected_database": "alien", "query": "Could you scan our database for potential signals matching Narrowband Technological Marker profiles? I need the signal fingerprints - ID, central frequency, frequency drift, Bandwidth-Frequency Ratio and the classification of NTM categories based on spectral coherence.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["/* \n-- Intent: Find signals matching Narrowband Technological Marker criteria with spectrum stability\n-- Uses KB item 15 (NTM) and advanced LATERAL JOIN for computational efficiency\n*/\n-- Step 1: Calculate BFR using a lateral join to improve query performance\nSELECT \n    s.SignalRegistry,\n    s.CenterFreqMhz,\n    s.FreqDriftHzs,\n    -- Calculate BFR using lateral join results\n    nbcalc.BFR,\n    -- Using CASE for categorization based on NTM criteria\n    CASE \n        WHEN nbcalc.BFR < 0.0001 AND s.FreqDriftHzs < 0.1 \n        THEN 'Strong NTM'\n        WHEN nbcalc.BFR < 0.0005 AND s.FreqDriftHzs < 0.5 \n        THEN 'Moderate NTM'\n        ELSE 'Not NTM'\n    END AS ntm_classification\nFROM Signals s\n-- Step 2: Use LATERAL JOIN for efficient calculation of BFR\nCROSS JOIN LATERAL (\n    SELECT (s.BwHz/(s.CenterFreqMhz * 1000000)) AS BFR\n) AS nbcalc\n-- Step 3: Apply primary NTM criteria from KB item 15\nWHERE \n    nbcalc.BFR < 0.001 AND\n    s.FreqDriftHzs < 1.0;"], "external_knowledge": [15, 39], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false}, "amb_user_query": "Could you scan our database for potential signals matching narrowband profiles? I need the signal identifiers, central frequency, drift rate, bandwidth ratio and the classification of NTM categories based on signal stability.", "user_query_ambiguity": {"critical_ambiguity": [{"term": "narrowband profiles", "sql_snippet": "WHERE nbcalc.BFR < 0.001 AND s.FreqDriftHzs < 1.0 as NTM", "is_mask": true, "type": "knowledge_linking_ambiguity"}, {"term": "signal stability", "sql_snippet": "s.FreqDriftHzs", "is_mask": false, "type": "schema_linking_ambiguity"}], "non_critical_ambiguity": []}, "knowledge_ambiguity": [{"term": "Narrowband Technological Marker (NTM)", "sql_snippet": "CASE WHEN nbcalc.BFR < 0.0001 AND s.FreqDriftHzs < 0.1 THEN 'Strong NTM' WHEN nbcalc.BFR < 0.0005 AND s.FreqDriftHzs < 0.5 THEN 'Moderate NTM' ELSE 'Not NTM' END AS ntm_classification", "is_mask": true, "type": "knowledge_ambiguity", "deleted_knowledge": 15}], "follow_up": {"query": "Instead of only focusing on Narrowband Technological Markers, compare them with Fast Radio Transients (FRTs) in the same output format, showing the signal ID, central frequency, frequency drift, Bandwidth-Frequency Ratio, and classify each signal as either an NTM category or FRT.", "sol_sql": "/* \n-- Intent: Compare signals matching Narrowband Technological Marker (NTM) profiles with Fast Radio Transients (FRTs)\n-- Uses KB item 15 (NTM), KB item 19 (FRT), and KB item 4 (BFR) for signal categorization\n*/\n-- Step 1: Calculate BFR and classify signals as NTM or FRT\nSELECT \n    s.SignalRegistry,\n    s.CenterFreqMhz,\n    s.FreqDriftHzs,\n    nbcalc.BFR,\n    -- Classify signal type (NTM or FRT)\n    CASE \n        WHEN nbcalc.BFR < 0.001 AND s.FreqDriftHzs < 1.0 THEN \n            CASE \n                WHEN nbcalc.BFR < 0.0001 AND s.FreqDriftHzs < 0.1 THEN 'Strong NTM'\n                WHEN nbcalc.BFR < 0.0005 AND s.FreqDriftHzs < 0.5 THEN 'Moderate NTM'\n                ELSE 'Not NTM'\n            END\n        WHEN s.SigDurSec < 0.1 AND s.SigStrDb > 15 AND s.BwHz > 1000000 AND sc.RepeatCount = 1 THEN 'FRT'\n either an NTM category or FRT, otherwise 'Others'\n    END AS signal_classification\nFROM Signals s\nLEFT JOIN SignalClassification sc ON s.SignalRegistry = sc.SignalRef  -- Join to get RepeatCount for FRT criteria\nCROSS JOIN LATERAL (\n    SELECT (s.BwHz/(s.CenterFreqMhz * 1000000)) AS BFR\n) AS nbcalc\n-- Step 2: Filter for signals that are either NTM or FRT\nWHERE \n    (nbcalc.BFR < 0.001 AND s.FreqDriftHzs < 1.0)  -- NTM criteria\n    OR (s.SigDurSec < 0.1 AND s.SigStrDb > 15 AND s.BwHz > 1000000 AND sc.RepeatCount = 1);  -- FRT criteria", "external_knowledge": [19], "type": "topic_pivot", "test_cases": [], "category": "Query", "difficulty_tier": "Hard"}, "difficulty_tier": "Easy"}
{"instance_id": "alien_9", "selected_database": "alien", "query": "Give me a ranked list of all our observatory sites by their current observation quality. For each observatory, I need fileds: the station name, telescope ID, calculated Observation Quality Factor, any equipment problems listed together sepearated by comma, how many issues there are, and the Observational Confidence Level. Please rank them by Observation Quality Factor with the best first.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["-- Intent: Evaluate combined observatory conditions and equipment status for observation quality\n-- Uses KB item 16 (OCL) and 32 (OQF), with advanced array functions\n-- Step 1: Calculate observation quality metrics using various observatory and telescope parameters\nWITH quality_assessment AS (\n    SELECT \n        o.ObservStation,\n        t.TelescRegistry,\n        o.AtmosTransparency,\n        o.HumidityRate,\n        o.WindSpeedMs,\n        o.LunarDistDeg,\n        t.EquipStatus,\n        t.CalibrStatus,\n        t.PointAccArc,\n        -- Calculate AOI from KB item 1\n        (o.AtmosTransparency * (1 - o.HumidityRate/100) * (1 - 0.02 * o.WindSpeedMs)) AS AOI,\n        -- Calculate LIF from KB item 9\n        ((1 - o.LunarDistDeg/180) * (1 - o.AtmosTransparency)) AS LIF,\n        -- Store equipment issues in array for analysis\n        ARRAY[\n            CASE WHEN t.EquipStatus != 'Degraded' THEN t.EquipStatus ELSE NULL END,\n            CASE WHEN t.CalibrStatus != 'Overdue' THEN t.CalibrStatus ELSE NULL END,\n            CASE WHEN t.CoolSysStatus != 'Warning' THEN t.CoolSysStatus ELSE NULL END\n        ] AS equipment_issues\n    FROM Observatories o\n    INNER JOIN Telescopes t ON o.ObservStation = t.ObservStation\n)\n-- Step 2: Apply OCL criteria and calculate OQF using conditional logic\nSELECT \n    qa.ObservStation,\n    qa.TelescRegistry,\n    -- Calculate OQF based on KB item 32\n    (qa.AOI * (1 - qa.LIF) * (CASE WHEN qa.PointAccArc < 2 THEN 1 ELSE 2/qa.PointAccArc END)) AS OQF,\n    -- Display equipment issues using array functions\n    ARRAY_TO_STRING(ARRAY_REMOVE(qa.equipment_issues, NULL), ', ') AS identified_issues,\n    -- Count equipment issues using array functions\n    CARDINALITY(ARRAY_REMOVE(qa.equipment_issues, NULL)) AS issue_count,\n    -- Apply OCL classification criteria from KB item 16\n    CASE \n        WHEN qa.AOI > 0.8 AND qa.EquipStatus = 'Operational' AND qa.CalibrStatus = 'Current' \n        THEN 'High'\n        WHEN qa.AOI BETWEEN 0.5 AND 0.8 \n        THEN 'Medium'\n        ELSE 'Low'\n    END AS ocl_classification\nFROM quality_assessment qa\n-- Step 3: Order by calculated quality for prioritization\nORDER BY OQF DESC;"], "external_knowledge": [16, 32, 55], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": -1, "distinct": false}, "amb_user_query": "Give me a ranked list of all our observation sites by their quality score. For each site, I need the name, telescope ID, calculated score, any equipment problems listed together, how many issues there are, and the confidence level. Please oder them.", "user_query_ambiguity": {"critical_ambiguity": [{"term": "observation sites", "sql_snippet": "FROM Observatories o INNER JOIN Telescopes t ON o.ObservStation = t.ObservStation", "is_mask": false, "type": "schema_linking_ambiguity"}, {"term": "quality score", "sql_snippet": "(qa.AOI * (1 - qa.LIF) * (CASE WHEN qa.PointAccArc < 2 THEN 1 ELSE 2/qa.PointAccArc END)) AS OQF", "is_mask": true, "type": "knowledge_linking_ambiguity"}, {"term": "listed together", "sql_snippet": "ARRAY_TO_STRING(ARRAY_REMOVE(qa.equipment_issues, NULL), ', ') AS identified_issues", "is_mask": false, "type": "intent_ambiguity"}], "non_critical_ambiguity": [{"term": "order", "sql_snippet": "ORDER BY OQF DESC", "is_mask": false, "type": "sort_ambiguity"}]}, "knowledge_ambiguity": [{"term": "Observational Confidence Level (OCL)", "sql_snippet": "  CASE WHEN qa.aoi > 0.8 AND qa.equipstatus = 'Operational' AND qa.calibrstatus = 'Current' THEN 'High' WHEN qa.aoi BETWEEN 0.5 AND 0.8 THEN 'Medium' ELSE 'Low' END AS ocl_classification", "is_mask": true, "type": "knowledge_ambiguity", "deleted_knowledge": 16}], "follow_up": {"query": "Compare the observation quality between observatories with 'Clear' and 'Cloudy' weather profiles. Keep the same output fields.", "sol_sql": "/* \n-- Intent: Compare observation quality between observatories with Clear vs. Cloudy weather profiles\n-- Uses KB item 16 (OCL), 32 (OQF), and 20 (WeathProfile: Clear) for weather profile context\n-- Step 1: Calculate observation quality metrics using various observatory and telescope parameters\n*/\nWITH quality_assessment AS (\n    SELECT \n        o.ObservStation,\n        t.TelescRegistry,\n        o.WeathProfile,\n        o.AtmosTransparency,\n        o.HumidityRate,\n        o.WindSpeedMs,\n        o.LunarDistDeg,\n        t.EquipStatus,\n        t.CalibrStatus,\n        t.PointAccArc,\n        -- Calculate AOI from KB item 1\n        (o.AtmosTransparency * (1 - o.HumidityRate/100) * (1 - 0.02 * o.WindSpeedMs)) AS AOI,\n        -- Calculate LIF from KB item 9\n        ((1 - o.LunarDistDeg/180) * (1 - o.AtmosTransparency)) AS LIF,\n        -- Store equipment issues in array for analysis\n        ARRAY[\n            CASE WHEN t.EquipStatus != 'Degraded' THEN t.EquipStatus ELSE NULL END,\n            CASE WHEN t.CalibrStatus != 'Overdue' THEN t.CalibrStatus ELSE NULL END,\n            CASE WHEN t.CoolSysStatus != 'Warning' THEN t.CoolSysStatus ELSE NULL END\n        ] AS equipment_issues\n    FROM Observatories o\n    INNER JOIN Telescopes t ON o.ObservStation = t.ObservStation\n    WHERE o.WeathProfile IN ('Clear', 'Cloudy')  -- Filter for Clear and Cloudy weather profiles\n)\n-- Step 2: Apply OCL criteria, calculate OQF, and compare by weather profile\nSELECT \n    qa.ObservStation,\n    qa.TelescRegistry,\n    qa.WeathProfile,\n    -- Calculate OQF based on KB item 32\n    (qa.AOI * (1 - qa.LIF) * (CASE WHEN qa.PointAccArc < 2 THEN 1 ELSE 2/qa.PointAccArc END)) AS OQF,\n    -- Display equipment issues using array functions\n    ARRAY_TO_STRING(ARRAY_REMOVE(qa.equipment_issues, NULL), ', ') AS identified_issues,\n    -- Count equipment issues using array functions\n    CARDINALITY(ARRAY_REMOVE(qa.equipment_issues, NULL)) AS issue_count,\n    -- Apply OCL classification criteria from KB item 16\n    CASE \n        WHEN qa.AOI > 0.8 AND qa.EquipStatus = 'Operational' AND qa.CalibrStatus = 'Current' \n        THEN 'High'\n        WHEN qa.AOI BETWEEN 0.5 AND 0.8 \n        THEN 'Medium'\n        ELSE 'Low'\n    END AS ocl_classification\nFROM quality_assessment qa\n-- Step 3: Order by weather profile and then by OQF for comparison\nORDER BY qa.WeathProfile, OQF DESC;", "external_knowledge": [20], "type": "topic_pivot", "test_cases": [], "category": "Query", "difficulty_tier": "Medium"}, "difficulty_tier": "Medium"}
{"instance_id": "alien_10", "selected_database": "alien", "query": "I want to find signals that might contain structured information by analyzing their stability patterns. For each candidate, show the signal ID, signal type, Signal Stability Metric, one field that contains a combined stability textual report formatted exactly as: \"Frequency: <frequency stability>, Phase: <phase stability>, Integrity: <signal integrity>\" and the CIP Classification Label.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["-- Intent: Identify signals showing evidence of coherent information patterns based on stability\n-- Uses KB item 7 (SSM) and 11 (CIP), with advanced string aggregation\n-- Step 1: Calculate Signal Stability Metric (SSM) using mathematical operations\nWITH stability_analysis AS (\n    SELECT \n        s.SignalRegistry,\n        s.SignalClass,\n        s.FreqDriftHzs,\n        s.FreqMhz,\n        s.SigDurSec,\n        s.DoppShiftHz,\n        s.ModType,\n        s.ModIndex,\n        sd.SigIntegrity,\n        sd.FreqStab,\n        sd.PhaseStab,\n        sc.EntropyVal,\n        -- Calculate SSM per KB item 7\n        ((1 - ABS(s.FreqDriftHzs)/(s.FreqMhz * 1000)) * \n         (s.SigDurSec/(1 + s.DoppShiftHz/1000))) AS SSM\n    FROM Signals s\n    INNER JOIN SignalDynamics sd ON s.SignalRegistry = sd.SignalRef\n    INNER JOIN SignalClassification sc ON s.SignalRegistry = sc.SignalRef\n)\n-- Step 2: Analyze coherent information patterns with string aggregation\nSELECT \n    sa.SignalRegistry,\n    sa.SignalClass,\n    ROUND(sa.SSM::numeric, 3) AS SSM,\n    -- Aggregate stability factors using string_agg\n    string_agg(\n        CASE \n            WHEN attribute = 'FreqStab' THEN 'Frequency: ' || value\n            WHEN attribute = 'PhaseStab' THEN 'Phase: ' || value\n            WHEN attribute = 'SigIntegrity' THEN 'Integrity: ' || value\n        END,\n        ', ' ORDER BY attribute\n    ) AS stability_factors,\n    -- Evaluate if signal meets CIP criteria from KB item 11\n    CASE \n        WHEN sa.SSM > 0.8 AND \n             sa.EntropyVal BETWEEN 0.4 AND 0.8 AND\n             sa.ModIndex > 0.5\n        THEN 'Coherent Information Pattern Detected'\n        WHEN sa.SSM > 0.6 AND sa.EntropyVal BETWEEN 0.3 AND 0.9\n        THEN 'Potential Information Pattern'\n        ELSE 'No Clear Pattern'\n    END AS pattern_assessment\nFROM stability_analysis sa\n-- Step 3: Use lateral join with unnest to transform stability attributes into rows for aggregation\nCROSS JOIN LATERAL (\n    SELECT unnest(ARRAY['FreqStab', 'PhaseStab', 'SigIntegrity']) AS attribute,\n           unnest(ARRAY[sa.FreqStab, sa.PhaseStab, sa.SigIntegrity]) AS value\n) AS stability_attrs\nGROUP BY \n    sa.SignalRegistry, sa.SignalClass, sa.SSM, sa.EntropyVal, sa.ModType, sa.ModIndex;"], "external_knowledge": [7, 11, 24], "test_cases": [], "category": "Query", "high_level": true, "conditions": {"decimal": 3, "distinct": false}, "amb_user_query": "I want to find signals that might contain structured data by analyzing their stability. For each candidate, show the ID, signal type, one field that contains a combined stability textual report to integrate (frequency stability, phase stability, and signal integrity), and the pattern Label.", "user_query_ambiguity": {"critical_ambiguity": [{"term": "pattern label", "sql_snippet": "CASE WHEN sa.ssm > 0.8 AND sa.entropyval BETWEEN 0.4 AND 0.8 AND sa.modindex > 0.5 THEN 'Coherent Information Pattern Detected' WHEN sa.ssm > 0.6 AND sa.entropyval BETWEEN 0.3 AND 0.9 THEN 'Potential Information Pattern' ELSE 'No Clear Pattern' END", "is_mask": true, "type": "semantic_ambiguity"}, {"term": "integrate", "sql_snippet": "    string_agg(\n        CASE \n            WHEN attribute = 'FreqStab' THEN 'Frequency: ' || value\n            WHEN attribute = 'PhaseStab' THEN 'Phase: ' || value\n            WHEN attribute = 'SigIntegrity' THEN 'Integrity: ' || value\n        END,\n        ', ' ORDER BY attribute\n    ) AS stability_factors,", "is_mask": false, "type": "intent_ambiguity"}], "non_critical_ambiguity": []}, "knowledge_ambiguity": [{"term": "CIP Classification Label", "sql_snippet": "SELECT sa.signalregistry,sa.signalclass,ROUND(sa.ssm::numeric, 3) AS ssm,string_agg(CASE WHEN attribute = 'FreqStab' THEN 'Frequency: ' || value WHEN attribute = 'PhaseStab' THEN 'Phase: ' || value WHEN attribute = 'SigIntegrity' THEN 'Integrity: ' || value END,', ') AS stability_factors,CASE WHEN sa.ssm > 0.8 AND sa.entropyval BETWEEN 0.4 AND 0.8 AND sa.modindex > 0.5 THEN 'Coherent Information Pattern Detected'WHEN sa.ssm > 0.6 AND sa.entropyval BET WEEN 0.3 AND 0.9 THEN 'Potential Information Pattern' ELSE 'No Clear Pattern' END AS pattern_assessment", "is_mask": true, "type": "knowledge_ambiguity", "deleted_knowledge": 24}], "follow_up": {"query": "Show only signals with very high stability (SSM > 0.8) that might indicate high-confidence technosignatures.", "sol_sql": "WITH stability_analysis AS (\n    SELECT \n        s.SignalRegistry,\n        s.SignalClass,\n        s.FreqDriftHzs,\n        s.FreqMhz,\n        s.SigDurSec,\n        s.DoppShiftHz,\n        s.ModType,\n        s.ModIndex,\n        sd.SigIntegrity,\n        sd.FreqStab,\n        sd.PhaseStab,\n        sc.EntropyVal,\n        ((1 - ABS(s.FreqDriftHzs)/(s.FreqMhz * 1000)) * \n         (s.SigDurSec/(1 + s.DoppShiftHz/1000))) AS SSM\n    FROM Signals s\n    INNER JOIN SignalDynamics sd ON s.SignalRegistry = sd.SignalRef\n    INNER JOIN SignalClassification sc ON s.SignalRegistry = sc.SignalRef\n)\nSELECT \n    sa.SignalRegistry,\n    sa.SignalClass,\n    ROUND(sa.SSM::numeric, 3) AS SSM,\n    string_agg(\n        CASE \n            WHEN attribute = 'FreqStab' THEN 'Frequency: ' || value\n            WHEN attribute = 'PhaseStab' THEN 'Phase: ' || value\n            WHEN attribute = 'SigIntegrity' THEN 'Integrity: ' || value\n        END,\n        ', '\n    ) AS stability_factors,\n    CASE \n        WHEN sa.SSM > 0.8 AND \n             sa.EntropyVal BETWEEN 0.4 AND 0.8 AND\n             sa.ModIndex > 0.5\n        THEN 'Coherent Information Pattern Detected'\n        WHEN sa.SSM > 0.6 AND sa.EntropyVal BETWEEN 0.3 AND 0.9\n        THEN 'Potential Information Pattern'\n        ELSE 'No Clear Pattern'\n    END AS pattern_assessment\nFROM stability_analysis sa\nCROSS JOIN LATERAL (\n    SELECT unnest(ARRAY['FreqStab', 'PhaseStab', 'SigIntegrity']) AS attribute,\n           unnest(ARRAY[sa.FreqStab, sa.PhaseStab, sa.SigIntegrity]) AS value\n) AS stability_attrs\nWHERE sa.SSM > 0.8  -- Changed from 0.6 to 0.8 to only show high stability signals\nGROUP BY \n    sa.SignalRegistry, sa.SignalClass, sa.SSM, sa.EntropyVal, sa.ModType, sa.ModIndex;", "external_knowledge": [], "type": "constraint_change", "test_cases": [], "category": "Query", "difficulty_tier": "Easy"}, "difficulty_tier": "Medium"}
{"instance_id": "alien_M_1", "selected_database": "alien", "query": "Flag all signals with poor quality by updating their SignalClass to 'Weak' when they have a negative Signal-to-Noise Quality Indicator (SNQI) value.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["-- Update Signals to flag low-quality detections.\n-- Intent: Mark signals with a computed Signal-to-Noise Quality Indicator (SNQI) below 0 by setting their SignalClass to 'Weak'.\n-- Step 1: Compute SNQI inline using the expression: SNQI = SnrRatio - 0.1 * ABS(NoiseFloorDbm).\n-- Step 2: Use the WHERE clause to filter records with SNQI < 0.\n-- Knowledge Used: \"Signal-to-Noise Quality Indicator (SNQI)\" [KB id:0]\nUPDATE Signals\nSET SignalClass = 'Weak'\nWHERE (SnrRatio - 0.1 * ABS(NoiseFloorDbm)) < 0;\n"], "external_knowledge": [0], "test_cases": ["\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Define the verification SQL to check that no row with SNQI < 0 remains unflagged.\n    verification_sql = (\n        \"SELECT COUNT(*) FROM Signals \"\n        \"WHERE (SnrRatio - 0.1 * ABS(NoiseFloorDbm)) < 0 \"\n        \"AND SignalClass <> 'Weak';\"\n    )\n    \n    # Execute the verification SELECT query.\n    pred_result, _, _ = execute_queries(verification_sql, db_name, conn)\n\n    # Expect pred_result to be a list with one tuple containing the count.\n    # Use assert to check that the count is zero.\n    # For example, if pred_result = [(0,)], then pred_result[0][0] should be 0.\n    assert pred_result[0][0] == 0, (\n        f\"Predicted SQL update did not update correctly: \"\n        f\"{pred_result[0][0]} rows still have SNQI < 0 without 'Weak' flag.\"\n    )\n    "], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false}, "amb_user_query": "Mark all signals with substandard quality by switching their class to 'Weak' if their quality metric is too low.", "user_query_ambiguity": {"critical_ambiguity": [{"term": "too low", "sql_snippet": "WHERE (SnrRatio - 0.1 * ABS(NoiseFloorDbm)) < 0", "is_mask": true, "type": "semantic_ambiguity"}, {"term": "class", "sql_snippet": "SET SignalClass = 'Weak'", "is_mask": false, "type": "schema_linking_ambiguity"}, {"term": "quality metric", "sql_snippet": "WHERE (SnrRatio - 0.1 * ABS(NoiseFloorDbm)) < 0", "is_mask": true, "type": "knowledge_linking_ambiguity"}], "non_critical_ambiguity": []}, "knowledge_ambiguity": [], "follow_up": {"query": "How many signals were flagged as 'Weak'?", "sol_sql": ["SELECT COUNT(*) AS weak_signal_count FROM Signals WHERE SignalClass = 'Weak';"], "external_knowledge": [], "test_cases": [], "type": "result_based", "category": "Query", "difficulty_tier": "Easy"}, "difficulty_tier": "Easy"}
{"instance_id": "alien_M_2", "selected_database": "alien", "query": "Create a PostgreSQL function called 'calculate_disf' that computes the Detection Instrument Sensitivity Factor (DISF) and return the calculated value. The parameters of the function are: air temperature in °C, atmospheric transparency, relative humidity %, and lunar distance in degrees.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["-- Create a function to calculate Detection Instrument Sensitivity Factor (DISF).\n-- Intent: Define a function that computes DISF using telescope environmental data and then test it.\n-- Step 1: Create the function \"calculate_disf\" using PL/pgSQL.\n--         Inputs: AirTempC, AtmosTransparency, HumidityRate, LunarDistDeg.\n--         Calculation: DISF = (10 - |AirTempC - 15|/10) * AtmosTransparency * (1 - HumidityRate/200) * ((100 - LunarDistDeg)/100)\n-- Knowledge Used:\n--   \"Detection Instrument Sensitivity Factor (DISF)\" [KB id:5] and advanced procedural language features.\nCREATE OR REPLACE FUNCTION calculate_disf(\n    p_airtemp NUMERIC, \n    p_trans NUMERIC, \n    p_humidity NUMERIC, \n    p_lunar_deg NUMERIC\n) RETURNS NUMERIC AS $$\nDECLARE\n    v_disf NUMERIC;\nBEGIN\n    -- Calculate the DISF based on the given formula.\n    v_disf := (10 - ABS(p_airtemp - 15)/10) * p_trans * (1 - p_humidity/200) * ((100 - p_lunar_deg)/100);\n    RETURN v_disf;\nEND;\n$$ LANGUAGE plpgsql;\n"], "external_knowledge": [5], "test_cases": ["\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    # The expected DISF value computed for inputs: AirTempC = 20, p_trans = 0.95, p_humidity = 45, p_lunar_deg = 60.\n    # Calculation:\n    #   factor1 = 10 - ABS(20 - 15)/10 = 10 - 5/10 = 10 - 0.5 = 9.5\n    #   factor2 = 0.95\n    #   factor3 = 1 - 45/200 = 1 - 0.225 = 0.775\n    #   factor4 = (100 - 60)/100 = 40/100 = 0.4\n    # Expected DISF = 9.5 * 0.95 * 0.775 * 0.4 ≈ 2.79775\n    expected_disf = 2.79775\n    tolerance = 0.00001  # Allowable floating-point error\n    \n    # Execute a SELECT to call calculate_disf with given parameters.\n    pred_result, _, _ = execute_queries(\"SELECT calculate_disf(20, 0.95, 45, 60);\", db_name, conn)\n    # Expecting pred_result to be a list with one tuple containing the computed value.\n    predicted_value = float(pred_result[0][0])\n    \n    # Step 3: Assert that the predicted value matches the expected DISF.\n    assert abs(predicted_value - expected_disf) < tolerance, (\n        f\"Predicted DISF value {predicted_value} differs from expected {expected_disf}\"\n    )\n    "], "category": "Management", "high_level": false, "conditions": {"decimal": -1, "distinct": false}, "amb_user_query": "Develop a PL/pgSQL routine called 'calculate_disf' that computes the sensitivity factor and return the calculated value.", "user_query_ambiguity": {"critical_ambiguity": [{"term": "routine", "sql_snippet": "CREATE OR REPLACE FUNCTION calculate_disf(", "is_mask": false, "type": "intent_ambiguity"}, {"term": "parameters order", "sql_snippet": "CREATE OR REPLACE FUNCTION calculate_disf(\n    p_airtemp NUMERIC, \n    p_trans NUMERIC, \n    p_humidity NUMERIC, \n    p_lunar_deg NUMERIC\n)", "is_mask": false, "type": "intent_ambiguity"}, {"term": "sensitivity factor", "sql_snippet": "v_disf := (10 - ABS(p_airtemp - 15)/10) * p_trans * (1 - p_humidity/200) * ((100 - p_lunar_deg)/100)", "is_mask": true, "type": "knowledge_linking_ambiguity"}], "non_critical_ambiguity": []}, "knowledge_ambiguity": [{"term": "Detection Instrument Sensitivity Factor (DISF)", "sql_snippet": "v_disf := (10 - ABS(p_airtemp - 15)/10) * p_trans * (1 - p_humidity/200) * ((100 - p_lunar_deg)/100)", "is_mask": true, "type": "knowledge_ambiguity", "deleted_knowledge": 5}], "follow_up": {"query": "Can you modify the function and add an optional minimum threshold parameter (defaulting to 0) to ensure the returned DISF value is never below this threshold?", "sol_sql": "\n    CREATE OR REPLACE FUNCTION calculate_disf(\n    p_airtemp NUMERIC, \n    p_trans NUMERIC, \n    p_humidity NUMERIC, \n    p_lunar_deg NUMERIC,\n    p_min_value NUMERIC DEFAULT 0\n) RETURNS NUMERIC AS $$\nDECLARE\n    v_disf NUMERIC;\nBEGIN\n    -- Calculate the DISF based on the given formula.\n    v_disf := (10 - ABS(p_airtemp - 15)/10) * p_trans * (1 - p_humidity/200) * ((100 - p_lunar_deg)/100);\n    \n    -- Return the maximum of calculated value or minimum threshold\n    RETURN GREATEST(v_disf, p_min_value);\nEND;\n$$ LANGUAGE plpgsql;", "external_knowledge": [], "test_cases": ["\n    def test_case(pred_sqls, sol_sqls, db_name, conn):      \n    # Test normal case\n    result1 = execute_queries(\"SELECT calculate_disf(20, 0.8, 50, 90, 5)\", db_name, conn)[0]\n    assert result1[0][0] == 5, \"Function should respect minimum threshold\"\n    \n    # Test case where calculated value is above threshold\n    result2 = execute_queries(\"SELECT calculate_disf(15, 1.0, 0, 100, -1)\", db_name, conn)[0]\n    assert result2[0][0] == 0, \"Function should return calculated value when above threshold\"\n    \n    # Test edge case\n    result3 = execute_queries(\"SELECT calculate_disf(-100, 0.1, 100, 0, 0)\", db_name, conn)[0]\n    assert result3[0][0] == 0, \"Function should handle edge cases correctly\"\n    "], "type": "constraint_change", "category": "Management", "difficulty_tier": "Easy"}, "difficulty_tier": "Easy"}
{"instance_id": "alien_M_3", "selected_database": "alien", "query": "Create a view called vw_observation_quality that calculates and displays the Observation Quality Factor (OQF) for each signal detection. Please include the observatory station, telescope registry, signal registry, along with the computed AOI, LIF, and OQF values in the view output.", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["-- Create a view to show observation quality factors.\n-- Intent: Join Observatories, Telescopes, and Signals to compute an \"Observation Quality Factor (OQF)\"\n--         that is based on AOI, LIF, and pointing accuracy.\n--\n-- Step 1: Use a CTE to join three tables using INNER JOIN.\n-- Step 2: Compute AOI = AtmosTransparency * (1 - HumidityRate/100) * (1 - 0.02 * WindSpeedMs).\n-- Step 3: Compute LIF = (1 - LunarDistDeg/180) * (1 - AtmosTransparency).\n-- Step 4: Compute OQF = AOI * (1 - LIF) * (CASE WHEN PointAccArc < 2 THEN 1 ELSE 2/PointAccArc END).\n--\n-- Advanced features used: CASE expressions, arithmetic operations, and a CTE for cleaner joins.\n--\n-- Knowledge Used:\n--   \"Atmospheric Observability Index (AOI)\" [KB id:1],\n--   \"Lunar Interference Factor (LIF)\" [KB id:9],\n--   and \"Observation Quality Factor (OQF)\" [KB id:32].\nCREATE OR REPLACE VIEW vw_observation_quality AS\nWITH calc AS (\n  SELECT\n    o.ObservStation,\n    t.TelescRegistry,\n    s.SignalRegistry,\n    o.AtmosTransparency,\n    o.HumidityRate,\n    o.WindSpeedMs,\n    o.LunarDistDeg,\n    t.PointAccArc,\n    -- Compute AOI based on environmental conditions.\n    (o.AtmosTransparency * (1 - o.HumidityRate/100.0) * (1 - 0.02 * o.WindSpeedMs)) AS aoi,\n    -- Compute LIF based on lunar distance and transparency.\n    ((1 - o.LunarDistDeg/180.0) * (1 - o.AtmosTransparency)) AS lif\n  FROM Observatories o\n  INNER JOIN Telescopes t ON o.ObservStation = t.ObservStation\n  INNER JOIN Signals s ON s.TelescRef = t.TelescRegistry\n)\nSELECT\n  ObservStation,\n  TelescRegistry,\n  SignalRegistry,\n  aoi,\n  lif,\n  aoi * (1 - lif) * (CASE WHEN PointAccArc < 2 THEN 1 ELSE 2.0/PointAccArc END) AS oqf\nFROM calc;\n"], "external_knowledge": [1, 9, 32], "test_cases": ["\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    tolerance = 0.0001  # Allowable floating point difference\n    # Define a verification query that joins the view with base tables to compute expected oqf.\n    verification_sql = \"\"\"\n    SELECT \n        v.oqf AS view_oqf,\n        (calc.aoi * (1 - calc.lif) * \n            (CASE WHEN calc.PointAccArc < 2 THEN 1 ELSE 2.0/calc.PointAccArc END)\n        ) AS expected_oqf\n    FROM vw_observation_quality v\n    JOIN (\n        SELECT\n            o.ObservStation,\n            t.TelescRegistry,\n            s.SignalRegistry,\n            (o.AtmosTransparency * (1 - o.HumidityRate/100.0) * (1 - 0.02 * o.WindSpeedMs)) AS aoi,\n            ((1 - o.LunarDistDeg/180.0) * (1 - o.AtmosTransparency)) AS lif,\n            t.PointAccArc\n        FROM Observatories o\n        INNER JOIN Telescopes t ON o.ObservStation = t.ObservStation\n        INNER JOIN Signals s ON s.TelescRef = t.TelescRegistry\n    ) calc\n    ON v.ObservStation = calc.ObservStation\n    AND v.TelescRegistry = calc.TelescRegistry\n    AND v.SignalRegistry = calc.SignalRegistry;\n    \"\"\"\n    # Execute verification query.\n    pred_verification_result, _, _ = execute_queries(verification_sql, db_name, conn)\n    # For each returned row, assert that view_oqf is within tolerance of expected_oqf.\n    for row in pred_verification_result:\n        view_oqf = float(row[0])\n        expected_oqf = float(row[1])\n        assert abs(view_oqf - expected_oqf) < tolerance, (\n            f\"Predicted view error: For row with ObservStation/TelescRegistry/SignalRegistry, \"\n            f\"view oqf ({view_oqf}) does not match expected oqf ({expected_oqf}) within tolerance.\"\n        )\n    "], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false}, "amb_user_query": "Create a view called 'vw_observation_quality' that calculates and displays the observation quality for each signal detection. Include the observatory station, telescope, signal, along with the computed factors in the view output.", "user_query_ambiguity": {"critical_ambiguity": [{"term": "observation quality", "sql_snippet": "aoi * (1 - lif) * (CASE WHEN PointAccArc < 2 THEN 1 ELSE 2.0/PointAccArc END) AS oqf", "is_mask": true, "type": "knowledge_linking_ambiguity"}, {"term": "factors", "sql_snippet": "aoi, lif, aoi * (1 - lif) * (CASE WHEN PointAccArc < 2 THEN 1 ELSE 2.0/PointAccArc END) AS oqf", "is_mask": true, "type": "intent_ambiguity"}, {"term": "telescope", "sql_snippet": "t.TelescRegistry", "is_mask": false, "type": "schema_linking_ambiguity"}], "non_critical_ambiguity": []}, "knowledge_ambiguity": [{"term": "Lunar Interference Factor (LIF)", "sql_snippet": "((1 - o.LunarDistDeg/180.0) * (1 - o.AtmosTransparency)) AS lif", "is_mask": true, "type": "knowledge_ambiguity", "deleted_knowledge": 9}], "follow_up": {"query": "Can you create a new view 'vw_high_quality_observations' that only shows high quality observations where 'aoi' > 0.8 and 'lif' < 0.3?", "sol_sql": "\nCREATE OR REPLACE VIEW vw_high_quality_observations AS\nWITH calc AS (\n  SELECT\n    o.ObservStation,\n    t.TelescRegistry,\n    s.SignalRegistry,\n    o.AtmosTransparency,\n    o.HumidityRate,\n    o.WindSpeedMs,\n    o.LunarDistDeg,\n    t.PointAccArc,\n    (o.AtmosTransparency * (1 - o.HumidityRate/100.0) * (1 - 0.02 * o.WindSpeedMs)) AS aoi,\n    ((1 - o.LunarDistDeg/180.0) * (1 - o.AtmosTransparency)) AS lif\n  FROM Observatories o\n  INNER JOIN Telescopes t ON o.ObservStation = t.ObservStation\n  INNER JOIN Signals s ON s.TelescRef = t.TelescRegistry\n  WHERE (o.AtmosTransparency * (1 - o.HumidityRate/100.0) * (1 - 0.02 * o.WindSpeedMs)) > 0.8\n    AND ((1 - o.LunarDistDeg/180.0) * (1 - o.AtmosTransparency)) < 0.3\n)\nSELECT\n  ObservStation,\n  TelescRegistry,\n  SignalRegistry,\n  aoi,\n  lif,\n  aoi * (1 - lif) * (CASE WHEN PointAccArc < 2 THEN 1 ELSE 2.0/PointAccArc END) AS oqf\nFROM calc;", "external_knowledge": [], "test_cases": ["\ndef test_case(pred_sqls, sol_sqls, db_name, conn):      \n    # Test that the view was created\n    res, _, _ = execute_queries(\"SELECT COUNT(*) FROM vw_high_quality_observations\", db_name, conn)\n    assert res[0][0] >= 0, \"View should exist and return results\"\n    \n    # Test that the filtering conditions are properly applied\n    res, _, _ = execute_queries(\"SELECT COUNT(*) FROM vw_high_quality_observations WHERE aoi <= 0.8 OR lif >= 0.3 OR oqf IS NULL\", db_name, conn)\n    assert res[0][0] == 0, \"View should only contain high quality observations meeting all criteria\"\n    "], "type": "constraint_change", "category": "Management", "difficulty_tier": "Easy"}, "difficulty_tier": "Medium"}
{"instance_id": "alien_M_4", "selected_database": "alien", "query": "I need you to set up an automatic system that flags whether signals in our database are good enough to analyze. Please add a new True/False column called 'IsAnalyzable' to the 'Signals' table, create a smart trigger function 'set_analyzable_flag' that checks each signal's quality by calculating its Signal-to-Noise Quality Indicator (SNQI), then set up this function to run automatically whenever new signals are added or existing ones are updated (set IsAnalyzable to TRUE when SNQI > 0 and set to FALSE otherwise).", "preprocess_sql": [], "clean_up_sqls": [], "sol_sql": ["-- Create a trigger to auto-set an \"IsAnalyzable\" flag on Signals.\n-- Intent: Automatically mark signals as analyzable (true) if their computed SNQI (SnrRatio - 0.1*ABS(NoiseFloorDbm)) is greater than 0.\n-- Step 1: Alter the Signals table to add a new boolean column \"IsAnalyzable\".\n-- Step 2: Create a PL/pgSQL trigger function \"set_analyzable_flag\" to compute SNQI and set the flag.\n-- Step 3: Create a trigger on Signals that fires BEFORE INSERT or UPDATE to call the function.\n-- Advanced features used: ALTER TABLE WITH IF NOT EXISTS, PL/pgSQL trigger functions, CASE-less conditional logic, and DROP TRIGGER IF EXISTS.\n-- Knowledge Used: \"Analyzable Signals\" [KB id:50] and \"Signal-to-Noise Quality Indicator (SNQI)\" [KB id:0].\nALTER TABLE Signals\nADD COLUMN IF NOT EXISTS IsAnalyzable BOOLEAN;\nCOMMENT ON COLUMN Signals.IsAnalyzable IS 'Flag indicating if signal is analyzable; true if SNQI > 0, based on KB (Analyzable Signals [id:50])';\nCREATE OR REPLACE FUNCTION set_analyzable_flag() RETURNS TRIGGER AS $$\nBEGIN\n    -- Compute SNQI and set IsAnalyzable accordingly.\n    IF (NEW.SnrRatio - 0.1 * ABS(NEW.NoiseFloorDbm)) > 0 THEN\n        NEW.IsAnalyzable := TRUE;\n    ELSE\n        NEW.IsAnalyzable := FALSE;\n    END IF;\n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\nDROP TRIGGER IF EXISTS trg_set_analyzable ON Signals;\nCREATE TRIGGER trg_set_analyzable\nBEFORE INSERT OR UPDATE ON Signals\nFOR EACH ROW\nEXECUTE FUNCTION set_analyzable_flag();\n"], "external_knowledge": [0, 50], "test_cases": ["\ndef test_case(pred_sqls, sol_sqls, db_name, conn):\n    # Insert test data into the Signals table and verify the trigger logic.\n    insert_sql = \"\"\"\n    INSERT INTO Signals (SignalRegistry, SnrRatio, NoiseFloorDbm, TelescRef)\n    VALUES \n        ('S001', 10, -50, 'T4621'),  -- SNQI = 10 - 0.1 * |-50| = 5 (IsAnalyzable = TRUE)\n        ('S002', 3, -40, 'T4621');   -- SNQI = 3 - 0.1 * |-40| = -1 (IsAnalyzable = FALSE)\n    \"\"\"\n    execute_queries(insert_sql, db_name, conn)\n    verification_sql = \"\"\"\n    SELECT SignalRegistry, IsAnalyzable\n    FROM Signals\n    WHERE SignalRegistry IN ('S001', 'S002')\n    ORDER BY SignalRegistry;\n    \"\"\"\n    pred_result, _, _ = execute_queries(verification_sql, db_name, conn)\n    # Expected results based on SNQI computation.\n    expected_result = [\n        ('S001', True),  # SNQI > 0\n        ('S002', False)  # SNQI <= 0\n    ]\n    # Assert the predicted results match the expected results.\n    for actual, expected in zip(pred_result, expected_result):\n        actual_registry, actual_flag = actual\n        expected_registry, expected_flag = expected\n        assert actual_registry.strip() == expected_registry.strip() and actual_flag == expected_flag, (\n            f\"Predicted SQL trigger test failed: Expected {expected_result}, got {pred_result}\"\n        )\n    "], "category": "Management", "high_level": true, "conditions": {"decimal": -1, "distinct": false}, "amb_user_query": "I need you to set up an automatic system that flags whether signals in our database are good enough (TRUE/FALSE). Please add a new flag column to the 'Signals' table, and then create a trigger function 'set_analyzable_flag' that checks each signal's quality by calculating its quality indicator and sets up this function to run automatically whenever new signals are added or existing ones are updated.", "user_query_ambiguity": {"critical_ambiguity": [{"term": "good enough", "sql_snippet": "IF (NEW.SnrRatio - 0.1 * ABS(NEW.NoiseFloorDbm)) > 0 THEN\n        NEW.IsAnalyzable := TRUE;\n    ELSE\n        NEW.IsAnalyzable := FALSE;\n    END IF;", "is_mask": true, "type": "semantic_ambiguity"}, {"term": "flag column", "sql_snippet": "ADD COLUMN IF NOT EXISTS IsAnalyzable BOOLEAN", "is_mask": false, "type": "intent_ambiguity"}, {"term": "quality indicator", "sql_snippet": "NEW.SnrRatio - 0.1 * ABS(NEW.NoiseFloorDbm)", "is_mask": true, "type": "knowledge_linking_ambiguity"}], "non_critical_ambiguity": []}, "knowledge_ambiguity": [], "follow_up": {"query": "Can we make the quality threshold stricter? Use 0.15 times the noise floor and require SNQI to be greater than 0.5.", "sol_sql": "\nCREATE OR REPLACE FUNCTION set_analyzable_flag() RETURNS TRIGGER AS $$\nBEGIN\n    -- Compute SNQI and set IsAnalyzable with stricter threshold\n    IF (NEW.SnrRatio - 0.15 * ABS(NEW.NoiseFloorDbm)) > 0.5 THEN\n        NEW.IsAnalyzable := TRUE;\n    ELSE\n        NEW.IsAnalyzable := FALSE;\n    END IF;\n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n", "external_knowledge": [], "test_cases": ["\ndef test_case(pred_sqls, sol_sqls, db_name, conn):      \n    # Test that the function was updated with the correct threshold\n    query = \"SELECT prosrc FROM pg_proc WHERE proname = 'set_analyzable_flag';\"\n    result, _, _ = execute_queries(query, db_name, conn)\n    assert \"0.15\" in result[0][0] and \"0.5\" in result[0][0], \"Function was not updated with stricter threshold\"\n    ", "\ndef test_case(pred_sqls, sol_sqls, db_name, conn):      \n    # Insert test data into the Signals table and verify the trigger logic.\n    insert_sql = \"\"\"\n    INSERT INTO Signals (SignalRegistry, SnrRatio, NoiseFloorDbm, TelescRef)\n    VALUES \n        ('S001', 7.9, -50, 'T4621'),  -- SNQI = 7.9 - 0.15 * |-50| = 0.4 (IsAnalyzable = FALSE)\n        ('S002', 5, -40, 'T4621');   -- SNQI = 5 - 0.15 * |-40| = -1 (IsAnalyzable = FALSE)\n    \"\"\"\n    execute_queries(insert_sql, db_name, conn)\n\n    # Query the results to check if the trigger computed IsAnalyzable correctly.\n    verification_sql = \"\"\"\n    SELECT SignalRegistry, IsAnalyzable\n    FROM Signals\n    WHERE SignalRegistry IN ('S001', 'S002')\n    ORDER BY SignalRegistry;\n    \"\"\"\n    pred_result, _, _ = execute_queries(verification_sql, db_name, conn)\n\n    # Expected results based on SNQI computation.\n    expected_result = [\n        ('S001', False),   # SNQI > 0 → IsAnalyzable = TRUE\n        ('S002', False)   # SNQI <= 0 → IsAnalyzable = FALSE\n    ]\n\n    # Assert the predicted results match the expected results.\n    for actual, expected in zip(pred_result, expected_result):\n        actual_registry, actual_flag = actual\n        expected_registry, expected_flag = expected\n        assert actual_registry.strip() == expected_registry.strip() and actual_flag == expected_flag, (\n            f\"Predicted SQL trigger test failed: Expected {expected_result}, got {pred_result}\")\n"], "type": "constraint_change", "category": "Management", "difficulty_tier": "Medium"}, "difficulty_tier": "Easy"}
